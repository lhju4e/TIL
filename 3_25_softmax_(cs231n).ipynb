{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cs231n softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_loss_naive(W, X, y, reg):\n",
    "  \"\"\"\n",
    "  Softmax loss function, naive implementation (with loops)\n",
    "\n",
    "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
    "  of N examples.\n",
    "\n",
    "  Inputs:\n",
    "  - W: A numpy array of shape (D, C) containing weights.\n",
    "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
    "    that X[i] has label c, where 0 <= c < C.\n",
    "  - reg: (float) regularization strength\n",
    "\n",
    "  Returns a tuple of:\n",
    "  - loss as single float\n",
    "  - gradient with respect to weights W; an array of same shape as W\n",
    "  \"\"\"\n",
    " # loss랑 dW 0으로 초기화\n",
    "  loss = 0.0\n",
    "  dW = np.zeros_like(W)\n",
    "\n",
    "  #############################################################################                                                        \n",
    "#  할 것 : softmax loss랑 그거의 gradient를 loop써서 구현해라\n",
    "#  loss랑 gradient를 dW에 저장해라\n",
    "#  만약에 니가 이거를 신경쓰지 않으면, numeric instability에 부딪히기 쉽다\n",
    "#  정규화 까먹지마라!\n",
    "#  #############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  for i in range(X.shape[0]):\n",
    "    score = np.dot(X[i], W) #score함수가 y = f(x, W) = xW #강의에 나옴(3강). 지금은 linear분류문제 하는중.\n",
    "    score -= np.max(score) #이거 왜해주는지 모르겠음 => 오버플로우를 줄이려고. 이것도 강의에 나옴.\n",
    "  # print(score.shape) (10,)\n",
    "\n",
    "    p = np.exp(score)/np.sum(np.exp(score)) #softmax공식\n",
    "  #print(p.shape) (10,)\n",
    "\n",
    "    loss += -np.log(p[y[i]])\n",
    "#print(loss.shape) ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss 공식이 \n",
    "loss =  $(-\\sum_{j} y_j log p_j)$\n",
    "\n",
    "인데, 이 공식에서의 $(y_j)$ 는 예를 들어 [1, 0, 0] [0, 1, 0] 이렇게.. 생겼다.\n",
    "\n",
    "결국 정답 레이블에 해당하는 요소만 $(log)$를 곱해준다.\n",
    "\n",
    "우리 코드에서의 y는 정답 클래스를 가리키는 값이다. y[0] = 0, y[1] = 1 이렇게..\n",
    "\n",
    "따라서 y[i] 값(정답 클래스 인덱스)를 p의 인덱스로 사용하면 정답 클래스에만 $(log)$를 곱해주는 것이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     p[y[i]] -= 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/https://ratsgo.github.io/deep%20learning/2017/10/02/softmax/\n",
    "\n",
    "\n",
    "이 분 블로그를 참고하여, softmax 노드의 그래디언트 값은, 결국 정답 클래스의 출력값에서 1만 빼주면 된다는 것을 알았다.\n",
    "\n",
    "p값은 어차피 for문 돌아가면서 다시 계산되니까, 그냥 이거에 -1 해주는것\n",
    "\n",
    "##### $(dw = P_i - y_i)$ 인데, y값은 위에서 말했다시피 [1, 0, 0] 이런식이므로, 결국 정답 클래스만 빼주는 거나 마찬가지\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       for j in range(W.shape[1]):\n",
    "            dW[:,j] += X[i, :] * p[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = xW 가 이 연산?인데, 여기서 dy/dW 하면 x가 남으니까, 계수 x에 구한 그래디언트값을 곱해줌\n",
    "\n",
    "더해주는거는, dW[ : , j ]니까 값이 한 열씩 들어감. 당연히 더해줘야함.\n",
    "\n",
    "이렇게 조각내놨지만.... 여기까지의 코드가 for i ...의 반복문에 포함되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "  loss /= X.shape[0] \n",
    "  dW /= X.shape[0]\n",
    " \n",
    " \n",
    "  # Add regularization to the loss. \n",
    "  loss += 0.5 * reg * np.sum(W * W) \n",
    "  dW += reg * W \n",
    "\n",
    "#내코드 끝나는줄\n",
    "  return loss, dW"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
